{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "# import torch.utils.data.dataloader\n",
    "import config\n",
    "\n",
    "#Policy Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, HIDDEN_DIM = config.HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        self.critic = nn.Linear(HIDDEN_DIM, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "    \n",
    "    def act(self, x):\n",
    "        state = torch.FloatTensor(x)\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1) \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "\n",
    "class PPOTrainer():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.policy = ActorCritic(self.state_dim, self.action_dim, config.HIDDEN_DIM)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.LR)\n",
    "        self.ep_rewards = []\n",
    "        self.best_avg = -np.inf\n",
    "    \n",
    "    def compute_advantage(self, rewards, next_values, values, dones):\n",
    "        deltas = rewards + config.GAMMA * next_values * (1 - dones) - values\n",
    "        advantages = np.zeros_like(deltas)\n",
    "        last_advantage = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            advantages[t] = deltas[t] + config.GAE_LAMBDA * config.GAMMA * (1-dones[t]) * last_advantage\n",
    "            last_advantage = advantages[t]\n",
    "        return (advantages - advantages.mean())/(advantages.std() + 1e-8)\n",
    "    \n",
    "    # log_probs, returns, advantages, states, actions\n",
    "    def update(self, old_log_probs, returns, advantages, states, old_actions):\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        old_actions = torch.LongTensor(old_actions)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(old_log_probs, old_actions,returns, advantages, states)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        for _ in range(config.NUM_EPOCHS):\n",
    "            for batch in loader:\n",
    "                old_lp, a, ret, adv, s = batch\n",
    "                \n",
    "                logits, values = self.policy(s)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                new_log_prob = dist.log_prob(a)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                ratio = (new_log_prob - old_lp).exp()\n",
    "                surr1 = ratio * adv\n",
    "                surr2 = torch.clamp(ratio, 1 - config.CLIP_EPS, 1 + config.CLIP_EPS) * adv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                value_loss = F.mse_loss(values.squeeze(), ret)\n",
    "                \n",
    "                loss = policy_loss + value_loss * config.VALUE_COEF - entropy * config.ENTROPY_COEF\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for episode in range(config.MAX_EPISODES):\n",
    "            states = [] \n",
    "            rewards = [] \n",
    "            actions = [] \n",
    "            dones = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            \n",
    "            for step in range(config.NUM_STEPS):\n",
    "                action, log_prob, value = self.policy.act(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                done = done or truncated\n",
    "                \n",
    "                states.append(state)\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                dones.append(done)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    state, _ = self.env.reset()\n",
    "                    self.ep_rewards.append(episode_reward)\n",
    "                    episode_reward = 0 \n",
    "                    \n",
    "            next_states = torch.FloatTensor(np.array([s for s in states]))\n",
    "            with torch.no_grad():\n",
    "                _, next_values = self.policy(next_states)\n",
    "            next_values = next_values.cpu().numpy().flatten()\n",
    "            next_values = np.append(next_values[1:], 0)\n",
    "            next_values[dones] = 0\n",
    "            \n",
    "            #Cpnverting to numpy arrays\n",
    "            rewards = np.array(rewards)\n",
    "            values = np.array(values)\n",
    "            dones = np.array(dones).astype(np.float32)\n",
    "            \n",
    "            advantages = self.compute_advantage(rewards, next_values, values, dones)\n",
    "            returns = advantages + values\n",
    "            \n",
    "            self.update(log_probs, returns, advantages, states, actions)\n",
    "            \n",
    "            avg_reward = np.mean(self.ep_rewards[-100:]) if len(self.ep_rewards) >= 100 else np.mean(self.ep_rewards)\n",
    "            if avg_reward > self.best_avg:\n",
    "                self.best_avg = avg_reward\n",
    "            print(f\"Episode {episode+1}, Reward: {self.ep_rewards[-1]:.4f}, Avg Reward: {avg_reward:.2f}\")\n",
    "            \n",
    "        torch.save(self.policy.state_dict(), 'ppo_model.pt')\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "    trainer = PPOTrainer(env)\n",
    "    trainer.train()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict loaded successfully from ppo_model.pt\n",
      "Episode 1: Total Reward = 273.33, Steps = 249\n",
      "Episode 2: Total Reward = 304.82, Steps = 247\n",
      "Episode 3: Total Reward = 277.60, Steps = 261\n",
      "Episode 4: Total Reward = 242.40, Steps = 641\n",
      "Episode 5: Total Reward = 250.88, Steps = 204\n",
      "Inference finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import os\n",
    "\n",
    "class MockConfig:\n",
    "    NUM_STEPS = 2048          # Number of steps per environment per update\n",
    "    BATCH_SIZE = 64           # Mini-batch size for updates\n",
    "    NUM_EPOCHS = 10           # Number of optimization epochs per update\n",
    "    GAMMA = 0.99              # Discount factor\n",
    "    GAE_LAMBDA = 0.95         # GAE parameter\n",
    "    CLIP_EPS = 0.2            # PPO clip parameter\n",
    "    LR = 3e-4                 # Learning rate\n",
    "    HIDDEN_DIM = 256          # Network hidden layer size\n",
    "    ENTROPY_COEF = 0.01       # Entropy coefficient\n",
    "    VALUE_COEF = 0.5          # Value loss coefficient\n",
    "    MAX_EPISODES = 300        # Maximum training episodes\n",
    "\n",
    "config = MockConfig()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, HIDDEN_DIM = config.HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        self.critic = nn.Linear(HIDDEN_DIM, 1) # Critic not used in inference, but part of the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "    \n",
    "    # To use deterministic action (argmax) or sampling based on preference\n",
    "    def act_inference(self, x, deterministic=True):\n",
    "        state = torch.FloatTensor(x).unsqueeze(0) # To add batch dimension\n",
    "        with torch.no_grad(): \n",
    "            logits, _ = self.forward(state)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(probs, dim=-1).item()\n",
    "            else:\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample().item()\n",
    "        return action\n",
    "\n",
    "\n",
    "def load_model(policy, filename='ppo_model.pt'):\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            state_dict = torch.load(filename)\n",
    "            # Load it into the policy network\n",
    "            policy.load_state_dict(state_dict)\n",
    "            # Setting the model to evaluation mode\n",
    "            policy.eval()\n",
    "            print(f\"Model state_dict loaded successfully from {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {filename}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Error: No model found at {filename}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"LunarLander-v3\" \n",
    "    env = gym.make(env_name, continuous=False, render_mode=\"human\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Instantiating the policy network\n",
    "    policy = ActorCritic(state_dim, action_dim, config.HIDDEN_DIM)\n",
    "\n",
    "    # Load the trained weights\n",
    "    model_loaded = load_model(policy, 'ppo_model.pt')\n",
    "\n",
    "    if model_loaded:\n",
    "        num_episodes = 5 # Number of episodes to run for testing\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "\n",
    "            while not done and not truncated:\n",
    "                # Choose deterministic=True for the 'best' action,\n",
    "                # or deterministic=False to sample like during training.\n",
    "                action = policy.act_inference(state, deterministic=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}, Steps = {step_count}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not load the model. Exiting inference.\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"Inference finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
